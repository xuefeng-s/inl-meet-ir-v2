{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Bert_and_sentiment_dict.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "history_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PascalBreuer/inl-meet-ir-v2/blob/Pascal/Bert_and_sentiment_dict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7SXEEmeoz95",
    "outputId": "7f4c34bd-cccf-401e-cd0f-9448b88d7d06"
   },
   "source": [
    "# Muss man in Google Colab einkommentieren\n",
    "#pip install transformers"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAzXG1uZlN0G",
    "outputId": "4eafa5cc-fa7d-4b99-9808-d14acc73f1cd"
   },
   "source": [
    "# Muss man in Google Colab einkommentieren\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "eaGId4_SkDd4",
    "outputId": "1e859d04-a320-4e26-9f32-fdfd032733ee"
   },
   "source": [
    "import re\n",
    "import sklearn\n",
    "import string\n",
    "import nltk\n",
    "import transformers as ppb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from enum import Enum\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import pipeline\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "print(pipeline('sentiment-analysis')('we love you'))\n",
    "\n",
    "\n",
    "# Man kann auch mehrere hintereinander machen\n",
    "\n",
    "class ColumnUser:\n",
    "    def set_column_to_use(self, column_name):\n",
    "        pass\n",
    "\n",
    "\n",
    "class ColumnTransformer(ColumnUser):\n",
    "    def set_column_to_transform(self, column_to_transform):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TextToSentenceTransformer(BaseEstimator, TransformerMixin, ColumnTransformer):\n",
    "    def __init__(self, column_to_transform, new_column_name):\n",
    "        self.column_to_transform = column_to_transform\n",
    "        self.new_column_name = new_column_name\n",
    "\n",
    "    def set_column_to_transform(self, column_to_transform):\n",
    "        self.column_to_transform = column_to_transform\n",
    "\n",
    "    def set_column_to_use(self, column_name):\n",
    "        self.new_column_name = column_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.column_to_transform in data.columns:\n",
    "            return pd.DataFrame({self.new_column_name: self.split_text_in_sentences(data)})\n",
    "        else:\n",
    "            self.log_error(\n",
    "                f'no column with name {self.column_to_transform} in dataframe.\\nGiving back original dataframe.')\n",
    "            return data\n",
    "\n",
    "    def split_text_in_sentences(self, data) -> List[str]:\n",
    "        texts = data[self.column_to_transform].tolist()\n",
    "        # delimiter = ['.', '\\?', '!']\n",
    "        delimiter = '[?.!]'\n",
    "        sentences = list()\n",
    "        for text in texts:\n",
    "            # sentences_in_text = [e + delimiter for e in text.split(delimiter) if e]\n",
    "            sentences_in_text = [e for e in re.split(delimiter, text) if e]\n",
    "            sentences += sentences_in_text\n",
    "        return sentences\n",
    "\n",
    "    def log_error(self, description, filename='logs/text_to_sentence_transformer.error'):\n",
    "        with open(filename, 'a', encoding='utf-8') as file:\n",
    "            file.write('#------------------------------------------------------------------------------------------\\n')\n",
    "            file.write(f'{datetime.now().strftime(\"%b-%d-%Y %H:%M:%S\")}\\n')\n",
    "            file.write(f'\\t{description}\\n')\n",
    "            file.write(\n",
    "                '#------------------------------------------------------------------------------------------\\n\\n\\n')\n",
    "\n",
    "\n",
    "class BertTransformer(BaseEstimator, TransformerMixin, ColumnUser):\n",
    "\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        model_class, tokenizer_class, pretrained_weights = (\n",
    "            ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "        # Load pretrained model/tokenizer\n",
    "\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "        self.model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "    def set_column_to_use(self, column_name):\n",
    "        self.column = column_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        # S채tze zerst체ckeln lassen\n",
    "        dataList = data[self.column].tolist()\n",
    "        dataList = list((str(s) for s in dataList))\n",
    "        tokenized = []\n",
    "\n",
    "        for s in dataList:\n",
    "            t = self.tokenizer.encode(s, add_special_tokens=True)\n",
    "            tokenized.append(t)\n",
    "            if len(t) > 500:\n",
    "                print('wtf oO')\n",
    "                print(s)\n",
    "                print(t)\n",
    "\n",
    "        # Padding hinzuf체gen\n",
    "        max_len = 0\n",
    "        for i in tokenized:\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "\n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
    "\n",
    "        # Maske erstellen, um das Padding bei der Verarbeitung zu filtern\n",
    "        mask = np.where(padded != 0, 1, 0)\n",
    "        mask.shape\n",
    "\n",
    "        # mache padded Array und Maske zu einem Tensor\n",
    "        # Tensor = mehrdimensionale Matrix mit einheitlichem Datentyp\n",
    "        input = torch.tensor(padded).to(torch.long).long()\n",
    "        mask = torch.tensor(mask).to(torch.long).long()\n",
    "\n",
    "        # gib unser Zeug an BERT\n",
    "        # no_grad = Angabe zur Simplifikation des Rechenvorgangs\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input, attention_mask=mask)\n",
    "\n",
    "        # nur die erste Spalte auslesen = von BERT geschriebene Kennwerte\n",
    "        features = output[0][:, 0, :].numpy()\n",
    "\n",
    "        return (data, features)\n",
    "\n",
    "\n",
    "class PreprocessorTransformer(BaseEstimator, TransformerMixin, ColumnUser):\n",
    "\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def set_column_to_use(self, column_name):\n",
    "        self.column = column_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_and_features, y=None):\n",
    "        data, features = data_and_features\n",
    "        sentences = data[self.column].tolist()\n",
    "        sentences = list((str(s) for s in sentences))\n",
    "\n",
    "        # muss vom generator object zur체ck zur liste gemacht werden\n",
    "        sentences = list((s.lower() for s in sentences))\n",
    "\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        sentences = [s.translate(table) for s in sentences]\n",
    "\n",
    "        sentences = [re.sub(r'\\d+', 'num', s) for s in sentences]\n",
    "\n",
    "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        sentences = [[word for word in s.split() if word not in stopwords] for s in sentences]\n",
    "        return (sentences, features)\n",
    "\n",
    "\n",
    "class SentimentOpinionValueCalculatorSingleValueTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, dict_name):\n",
    "        self.dict_name = dict_name\n",
    "        df = pd.read_csv(dict_name, sep=';')\n",
    "        self.value_dict = pd.Series(df.value.values, index=df.word).to_dict()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, sentences_and_features):\n",
    "        sentiment_opinion_scores = []\n",
    "        sentences, features = sentences_and_features\n",
    "        for sentence in sentences:\n",
    "            word_count = len(sentence)\n",
    "            sentiment_opinion_score = 0\n",
    "            if word_count > 0:\n",
    "                for word in sentence:\n",
    "                    if word in self.value_dict:\n",
    "                        sentiment_opinion_score = sentiment_opinion_score + self.value_dict[word]\n",
    "                sentiment_opinion_score = sentiment_opinion_score / word_count\n",
    "            sentiment_opinion_scores.append([sentiment_opinion_score])\n",
    "        for i in range(len(sentiment_opinion_scores)):\n",
    "            features[i] = features[i] + (sentiment_opinion_scores[i][0])\n",
    "        return features\n",
    "\n",
    "\n",
    "class SentimentOpinionValueCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, value_file_name):\n",
    "        df = pd.read_csv(value_file_name, sep=';')\n",
    "        self.value_dict = pd.Series(df.value.values, index=df.word).to_dict()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, sentences_and_features):\n",
    "        sentiment_opinion_scores = []\n",
    "        sentences, features = sentences_and_features\n",
    "        for sentence in sentences:\n",
    "            word_count = len(sentence)\n",
    "            sentiment_opinion_score = 0\n",
    "            if word_count > 0:\n",
    "                for word in sentence:\n",
    "                    if word in self.value_dict:\n",
    "                        sentiment_opinion_score += 1\n",
    "            sentiment_opinion_scores.append([sentiment_opinion_score])\n",
    "        for i in range(len(sentiment_opinion_scores)):\n",
    "            features[i] = features[i] + (sentiment_opinion_scores[i][0])\n",
    "        return features\n",
    "\n",
    "\n",
    "class PipelineRunner:\n",
    "\n",
    "    def __init__(self, dict_file, training_file, test_file, log_file='results/results_with_correct_input.log'):\n",
    "        self.dict_file = dict_file\n",
    "        self.log_file = log_file\n",
    "        self.data_training = pd.read_excel(training_file, sheet_name='sentences')\n",
    "        self.data_training.drop(\n",
    "            ['SUBJindl', 'SUBJsrce', 'SUBJrhet', 'SUBJster', 'SUBJspee', 'SUBJinspe', 'SUBJprop', 'SUBJpolit'],\n",
    "            axis=1,\n",
    "            inplace=True)\n",
    "        self.data_test = pd.read_excel(test_file, sheet_name='sentences')\n",
    "        self.data_test.drop(\n",
    "            ['SUBJindl', 'SUBJsrce', 'SUBJrhet', 'SUBJster', 'SUBJspee', 'SUBJinspe', 'SUBJprop', 'SUBJpolit'],\n",
    "            axis=1,\n",
    "            inplace=True)\n",
    "        self.pipeline = None\n",
    "        self.transformer_name_dict = {\n",
    "                                      TextToSentenceTransformer.__name__: \"ts\",\n",
    "                                      BertTransformer.__name__: \"bert\",\n",
    "                                      PreprocessorTransformer.__name__: \"prepro\",\n",
    "                                      SentimentOpinionValueCalculatorSingleValueTransformer.__name__: \"sentval\",\n",
    "                                      SentimentOpinionValueCounterTransformer.__name__: \"sentcount\"\n",
    "                                      }\n",
    "        self.estimator_name_dict = {\n",
    "            LogisticRegression.__name__: 'log_reg',\n",
    "            GaussianNB.__name__: 'gau_nb',\n",
    "            BernoulliNB.__name__: 'ber_nb'\n",
    "        }\n",
    "\n",
    "    def prepare_pipeline(self, data_column, estimator_type, transformer_types_list):\n",
    "\n",
    "        if not self.pipeline:\n",
    "            raise RuntimeError(\"Don't call this function directly use make_pipeline\")\n",
    "\n",
    "        print(f'Starting fitting: {estimator_type}')\n",
    "        accuracy = self.fit_and_predict_and_calculate_accuracy_pipe(data_column)\n",
    "        description = f'\\tUsed estimator: {estimator_type}\\n'\n",
    "        description += f'\\tUsed transformers: {\", \".join(transformer_types_list)}\\n'\n",
    "        description += f'\\tColumn: {data_column}\\n'\n",
    "        self.write_result_to_file(accuracy, description)\n",
    "\n",
    "    def make_pipeline(self, transformer_list, estimator,\n",
    "                      data_column, param_gird, classifier_description='',\n",
    "                      dir_path='', force_fitting=False):\n",
    "\n",
    "        classifier_file = self.create_pipe_line_name(transformer_list, estimator, classifier_description, data_column, dir_path)\n",
    "        if force_fitting or not os.path.exists(classifier_file):\n",
    "            print('No classfier found for your configuration or force_fitting=True. Creating new one and saving it.')\n",
    "            clf = GridSearchCV(estimator=estimator, param_grid=param_gird, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "            self.pipeline = sklearn.pipeline.Pipeline(\n",
    "                [(f'stage: {self.transformer_name_dict[type(transformer_list[index]).__name__]}',\n",
    "                  transformer_list[index]) for index in range(len(transformer_list))] +\n",
    "                [('clf', clf)]\n",
    "            )\n",
    "            self.prepare_pipeline(data_column, type(estimator).__name__, [type(t).__name__ for t in transformer_list])\n",
    "            print(f'Saving classifier to file {classifier_file}')\n",
    "            self.save_classifier(classifier_file)\n",
    "        else:\n",
    "            print(f'Classifier exists. Loaded from file {classifier_file}')\n",
    "            self.load_classifier(classifier_file)\n",
    "\n",
    "        return self.pipeline\n",
    "\n",
    "\n",
    "    def create_pipe_line_name(self, transformer_list, estimator, classifier_description, data_column, dir_path=''):\n",
    "        names = [self.transformer_name_dict[type(transformer).__name__] for transformer in transformer_list]\n",
    "        description = ''\n",
    "        if classifier_description:\n",
    "            description = '_' + classifier_description\n",
    "        if dir_path and not dir_path.endswith('/') and not dir_path.endswith('\\\\'):\n",
    "            dir_path += '/'\n",
    "        return dir_path + f'classifier/{self.estimator_name_dict[type(estimator).__name__]}_with_{\"_\".join(names)}{description}_{data_column}_pipeline.joblib.plk'\n",
    "\n",
    "    def load_classifier(self, filename):\n",
    "        self.pipeline = joblib.load(filename)\n",
    "\n",
    "    def save_classifier(self, filename):\n",
    "        joblib.dump(self.pipeline, filename)\n",
    "\n",
    "    def fit_and_predict_and_calculate_accuracy_pipe(self, data_column):\n",
    "        print('Start fiting')\n",
    "        self.pipeline.fit(self.data_training, self.data_training[data_column].to_numpy())\n",
    "        print('Start prediction')\n",
    "        start = time.time()\n",
    "        y_pred_pipe = self.pipeline.predict(self.data_test)\n",
    "        end = time.time()\n",
    "        print(f'time needed: {end - start}')\n",
    "        return accuracy_score(self.data_test[data_column].to_numpy(), y_pred_pipe)\n",
    "\n",
    "    def write_result_to_file(self, accuracy, description):\n",
    "        with open(self.log_file, 'a', encoding='utf-8') as file:\n",
    "            file.write('#------------------------------------------------------------------------------------------\\n')\n",
    "            file.write(f'{datetime.now().strftime(\"%b-%d-%Y %H:%M:%S\")}\\n')\n",
    "            file.write(f'{description}\\n')\n",
    "            file.write(f'\\tAccuracy: {accuracy}\\n')\n",
    "            file.write('#------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "    def predict_data(self, data_file_name, column_to_transform=None, new_column_name=None, batch_size=1, result_for_column='', log_file=f'results/results_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log'):\n",
    "        data_validation = pd.read_csv(data_file_name)\n",
    "\n",
    "        if column_to_transform is not None:\n",
    "            for obj in self.pipeline.named_steps.values():\n",
    "                if issubclass(type(obj), ColumnTransformer):\n",
    "                    obj.set_column_to_transform(column_to_transform)\n",
    "\n",
    "        if new_column_name is not None:\n",
    "            for obj in self.pipeline.named_steps.values():\n",
    "                if issubclass(type(obj), ColumnUser):\n",
    "                    obj.set_column_to_use(new_column_name)\n",
    "\n",
    "        with open(log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write('#-------------------------------------------------')\n",
    "            f.write(f'result for column {result_for_column}:\\n')\n",
    "\n",
    "        row_count = data_validation.shape[0]\n",
    "        counter = 0\n",
    "\n",
    "        while counter * batch_size < row_count:\n",
    "\n",
    "            output = self.pipeline.predict(data_validation.loc[counter * batch_size: (counter + 1) * batch_size])\n",
    "            with open(log_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(f'{output.tolist()}')\n",
    "                f.write('\\n')\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "        with open(log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write('#-------------------------------------------------\\n\\n\\n\\n\\n')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dir_path = ''\n",
    "# dict_file = dir_path + 'AFINN-both-abs.csv'\n",
    "dict_file = dir_path + 'sentiment_dict.csv'\n",
    "\n",
    "training_file = dir_path + 'Trainingdata_train.xlsx'\n",
    "\n",
    "test_file = dir_path + 'Trainingdata_test.xlsx'\n",
    "\n",
    "transformers_list = [TextToSentenceTransformer('text', 'Sentence'),\n",
    "                     BertTransformer('Sentence'),\n",
    "                     PreprocessorTransformer('Sentence'),\n",
    "                     SentimentOpinionValueCalculatorSingleValueTransformer(dict_file)]\n",
    "\n",
    "pipeline_runner = PipelineRunner(dict_file, training_file, test_file, log_file=dir_path + 'results/results_with_dict_only.log')\n",
    "Cs = np.logspace(-6, 6, 200)\n",
    "max_iter = 500\n",
    "log_reg_subjopin = LogisticRegression(max_iter=max_iter)\n",
    "pipeline_runner.make_pipeline(transformers_list, log_reg_subjopin, 'SUBJopin01', dict(C=Cs), dir_path=dir_path)\n",
    "\n",
    "pipeline_runner.predict_data(data_file_name=dir_path + 'MBFC_Dataset_Sample.csv',\n",
    "                             result_for_column='SUBJopin',\n",
    "                             log_file=dir_path + f'results/mbfc_results_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}_SUBJopin.log')\n",
    "\n",
    "log_reg_subjlang = LogisticRegression(max_iter=max_iter)\n",
    "pipeline_runner.make_pipeline(transformers_list, log_reg_subjlang, 'SUBJlang01', dict(C=Cs), dir_path=dir_path)\n",
    "pipeline_runner.predict_data(data_file_name=dir_path + 'MBFC_Dataset_Sample.csv',\n",
    "                             result_for_column='SUBJlang',\n",
    "                             log_file=dir_path + f'results/mbfc_results_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}_SUBJlang.log')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZC9OLXzMo4sC"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}