{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Bert_and_sentiment_dict.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "history_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PascalBreuer/inl-meet-ir-v2/blob/Pascal/Bert_and_sentiment_dict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7SXEEmeoz95",
    "outputId": "7f4c34bd-cccf-401e-cd0f-9448b88d7d06"
   },
   "source": [
    "# pip install transformers"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAzXG1uZlN0G",
    "outputId": "4eafa5cc-fa7d-4b99-9808-d14acc73f1cd"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "eaGId4_SkDd4",
    "outputId": "1e859d04-a320-4e26-9f32-fdfd032733ee"
   },
   "source": [
    "import re\n",
    "import sklearn\n",
    "import string\n",
    "import nltk\n",
    "import transformers as ppb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "print(pipeline('sentiment-analysis')('we love you'))\n",
    "\n",
    "\n",
    "# Man kann auch mehrere hintereinander machen\n",
    "\n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        model_class, tokenizer_class, pretrained_weights = (\n",
    "            ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "        # Load pretrained model/tokenizer\n",
    "\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "        self.model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        # Sätze zerstückeln lassen\n",
    "        dataList = data['Sentence'].tolist()\n",
    "        dataList = list((str(s) for s in dataList))\n",
    "        tokenized = []\n",
    "\n",
    "        for s in dataList:\n",
    "            tokenized.append(self.tokenizer.encode(s, add_special_tokens=True))\n",
    "\n",
    "        # Padding hinzufügen\n",
    "        max_len = 0\n",
    "        for i in tokenized:\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "\n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
    "\n",
    "        # Maske erstellen, um das Padding bei der Verarbeitung zu filtern\n",
    "        mask = np.where(padded != 0, 1, 0)\n",
    "        mask.shape\n",
    "\n",
    "        # mache padded Array und Maske zu einem Tensor\n",
    "        # Tensor = mehrdimensionale Matrix mit einheitlichem Datentyp\n",
    "        input = torch.tensor(padded).to(torch.long).long()\n",
    "        mask = torch.tensor(mask).to(torch.long).long()\n",
    "\n",
    "        # gib unser Zeug an BERT\n",
    "        # no_grad = Angabe zur Simplifikation des Rechenvorgangs\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input, attention_mask=mask)\n",
    "\n",
    "        # nur die erste Spalte auslesen = von BERT geschriebene Kennwerte\n",
    "        features = output[0][:, 0, :].numpy()\n",
    "\n",
    "        return (data, features)\n",
    "\n",
    "\n",
    "class PreprocessorTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_and_features, y=None):\n",
    "        data, features = data_and_features\n",
    "        sentences = data['Sentence'].tolist()\n",
    "        sentences = list((str(s) for s in sentences))\n",
    "\n",
    "        # muss vom generator object zurück zur liste gemacht werden\n",
    "        sentences = list((s.lower() for s in sentences))\n",
    "\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        sentences = [s.translate(table) for s in sentences]\n",
    "\n",
    "        sentences = [re.sub(r'\\d+', 'num', s) for s in sentences]\n",
    "\n",
    "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        sentences = [[word for word in s.split() if word not in stopwords] for s in sentences]\n",
    "        return (sentences, features)\n",
    "\n",
    "\n",
    "class SentimentOpinionValueCalculatorSingleValueTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, dict_name):\n",
    "        self.dict_name = dict_name\n",
    "        # print(f'name of dict: {dict_name}')\n",
    "        df = pd.read_csv(dict_name, sep=';')\n",
    "        self.value_dict = pd.Series(df.value.values, index=df.word).to_dict()\n",
    "        # for key in self.value_dict.keys():\n",
    "        #     print(f'{key} - {self.value_dict[key]}')\n",
    "        # print(f'unique values: {df.nunique()}')\n",
    "        # print(self.value_dict)\n",
    "        # print(len(self.value_dict))\n",
    "        # d = {}\n",
    "        # for word in df.word:\n",
    "        #     d[word] = df.value[df.word == word]\n",
    "        # for key in d.keys():\n",
    "        #     print(f'{key} - {d[key]}')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, sentences_and_features):\n",
    "        sentiment_opinion_scores = []\n",
    "        sentences, features = sentences_and_features\n",
    "        for sentence in sentences:\n",
    "            word_count = len(sentence)\n",
    "            # print(f'length of sentence {sentence} = {word_count}')\n",
    "            sentiment_opinion_score = 0\n",
    "            if word_count > 0:\n",
    "                for word in sentence:\n",
    "                    if word in self.value_dict:\n",
    "                        sentiment_opinion_score = sentiment_opinion_score + self.value_dict[word]\n",
    "                sentiment_opinion_score = sentiment_opinion_score / word_count\n",
    "            sentiment_opinion_scores.append([sentiment_opinion_score])\n",
    "        for i in range(len(sentiment_opinion_scores)):\n",
    "            features[i] = features[i] + (sentiment_opinion_scores[i][0])\n",
    "        return features\n",
    "\n",
    "\n",
    "class SentimentOpinionValueCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, value_file_name):\n",
    "        df = pd.read_csv(value_file_name, sep=';')\n",
    "        self.value_dict = pd.Series(df.value.values, index=df.word).to_dict()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, sentences_and_features):\n",
    "        sentiment_opinion_scores = []\n",
    "        sentences, features = sentences_and_features\n",
    "        for sentence in sentences:\n",
    "            word_count = len(sentence)\n",
    "            # print(f'length of sentence {sentence} = {word_count}')\n",
    "            sentiment_opinion_score = 0\n",
    "            if word_count > 0:\n",
    "                for word in sentence:\n",
    "                    if word in self.value_dict:\n",
    "                        sentiment_opinion_score += 1\n",
    "                # sentiment_opinion_score = sentiment_opinion_score / word_count\n",
    "            sentiment_opinion_scores.append([sentiment_opinion_score])\n",
    "        for i in range(len(sentiment_opinion_scores)):\n",
    "            features[i] = features[i] + (sentiment_opinion_scores[i][0])\n",
    "        return features\n",
    "\n",
    "\n",
    "class PipelineRunner:\n",
    "\n",
    "    def __init__(self, dict_file, training_file, test_file, log_file='results_with_correct_input.log'):\n",
    "        self.dict_file = dict_file\n",
    "        self.log_file = log_file\n",
    "        self.data_training = pd.read_excel(training_file, sheet_name='sentences')\n",
    "        self.data_training.drop(\n",
    "            ['SUBJindl', 'SUBJsrce', 'SUBJrhet', 'SUBJster', 'SUBJspee', 'SUBJinspe', 'SUBJprop', 'SUBJpolit'],\n",
    "            axis=1,\n",
    "            inplace=True)\n",
    "        self.data_test = pd.read_excel(test_file, sheet_name='sentences')\n",
    "        self.data_test.drop(\n",
    "            ['SUBJindl', 'SUBJsrce', 'SUBJrhet', 'SUBJster', 'SUBJspee', 'SUBJinspe', 'SUBJprop', 'SUBJpolit'],\n",
    "            axis=1,\n",
    "            inplace=True)\n",
    "        self.Cs = np.logspace(-6, 6, 200)\n",
    "\n",
    "    def start_all_pipelines(self, data_column):\n",
    "        transformer_list = [BertTransformer(),\n",
    "                            PreprocessorTransformer(),\n",
    "                            SentimentOpinionValueCalculatorSingleValueTransformer(dict_file)]\n",
    "\n",
    "        description = f'Bert und Sentiment Durchschnittswert (mit langem Dictonary). Spalte {data_column}'\n",
    "\n",
    "\n",
    "        print('Starting with Logistic Regression')\n",
    "\n",
    "        pipeline_to_use = self.make_pipeline(transformer_list, LogisticRegression(max_iter=500), dict(C=self.Cs))\n",
    "\n",
    "        log_reg_typ = \"Logistic Regression\"\n",
    "        accuracy = self.fit_and_predict_and_calculate_accuracy_pipe(pipeline_to_use, data_column)\n",
    "        self.write_result_to_file(accuracy, log_reg_typ, description)\n",
    "\n",
    "        # Die folgenden Zeilen kann man wieder einkommentiern, wenn man noch mal was an der Pipeline geändert hat oder testen möchte,\n",
    "        # ob mit mehr Testdaten einer der beiden Classifier besser ist. Ansonsten braucht man die nicht.\n",
    "        # print('Starting with Gaussian Naive Bayes')\n",
    "        #\n",
    "        # gau_nb_typ = \"Gaussian Naive Bayes\"\n",
    "        #\n",
    "        # pipeline_to_use = self.make_pipeline(transformer_list, GaussianNB(), dict(var_smoothing=self.Cs))\n",
    "        # accuracy = self.fit_and_predict_and_calculate_accuracy_pipe(pipeline_to_use, data_column)\n",
    "        # self.write_result_to_file(accuracy, gau_nb_typ, description)\n",
    "        #\n",
    "        #\n",
    "        # print('Starting with Bernoulli Naive Bayes')\n",
    "        #\n",
    "        # bernoulli_nb_typ = \"Bernoulli Naive Bayes\"\n",
    "        #\n",
    "        # pipeline_to_use = self.make_pipeline(transformer_list, BernoulliNB(), dict(alpha=self.Cs, binarize=self.Cs))\n",
    "        # accuracy = self.fit_and_predict_and_calculate_accuracy_pipe(pipeline_to_use, data_column)\n",
    "        # self.write_result_to_file(accuracy, bernoulli_nb_typ, description)\n",
    "\n",
    "    def make_pipeline(self, transformer_list, estimator, param_gird):\n",
    "        clf = GridSearchCV(estimator=estimator, param_grid=param_gird, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "        return sklearn.pipeline.Pipeline(\n",
    "            [(f'stage: {index}', transformer_list[index]) for index in range(len(transformer_list))] + [('clf', clf)]\n",
    "        )\n",
    "\n",
    "    def fit_and_predict_and_calculate_accuracy_pipe(self, pipe, data_column):\n",
    "        pipe.fit(self.data_training, self.data_training[data_column].to_numpy())\n",
    "\n",
    "        y_pred_pipe = pipe.predict(self.data_test)\n",
    "\n",
    "        return accuracy_score(self.data_test[data_column].to_numpy(), y_pred_pipe)\n",
    "\n",
    "    def write_result_to_file(self, accuracy, type, description):\n",
    "        with open(self.log_file, 'a', encoding='utf-8') as file:\n",
    "            file.write('#------------------------------------------------------------------------------------------\\n')\n",
    "            file.write(f'{datetime.now().strftime(\"%b-%d-%Y %H:%M:%S\")}\\n')\n",
    "            file.write(f'\\t{description}\\n')\n",
    "            file.write(f'\\t\\tAccuracy for classifier {type}: {accuracy}\\n')\n",
    "            file.write('#------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "\n",
    "\n",
    "def fit_and_predict_and_calculate_accuracy_pipe(pipe, train_input, train_ouput, test_input, test_output):\n",
    "    pipe.fit(train_input, train_ouput)\n",
    "\n",
    "    y_pred_pipe = pipe.predict(test_input)\n",
    "\n",
    "    return accuracy_score(y_pred_pipe, test_output)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Falls man das Dictonary in einem eigenen Ordner legen möchte (wenn man zum Beispiel mehrere Varianten hat)\n",
    "dict_dir_path = ''\n",
    "# dict_file = dir_path + 'AFINN-both-abs.csv'\n",
    "dict_file = dict_dir_path + 'sentiment_dict.csv'\n",
    "\n",
    "# Wenn man einen Ordner anlegen möchte kann man das hier machen\n",
    "data_dir_path = ''\n",
    "# Datei der Daten zum trainieren\n",
    "training_file = data_dir_path + 'Trainingdata_train.xlsx'\n",
    "# Datei der Daten zum testen\n",
    "test_file = data_dir_path + 'Trainingdata_test.xlsx'\n",
    "\n",
    "# Ordner für das Ergebnis\n",
    "result_dir_path = ''\n",
    "# Wo soll das Ergebnis gespeichert werden\n",
    "result_file = result_dir_path + 'results_with_correct_input.log'\n",
    "\n",
    "# Man muss jetzt nur noch dem PipelineRunner erzeugen und auf dem dann für jede Spalte die Methode starten\n",
    "pipeline_runner = PipelineRunner(dict_file, training_file, test_file)\n",
    "pipeline_runner.start_all_pipelines('SUBJopin01')\n",
    "pipeline_runner.start_all_pipelines('SUBJlang01')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZC9OLXzMo4sC"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}