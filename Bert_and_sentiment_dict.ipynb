{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Bert_and_sentiment_dict.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "history_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PascalBreuer/inl-meet-ir-v2/blob/Pascal/Bert_and_sentiment_dict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7SXEEmeoz95",
    "outputId": "7f4c34bd-cccf-401e-cd0f-9448b88d7d06"
   },
   "source": [
    "# Muss man in Google Colab einkommentieren\n",
    "#pip install transformers"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAzXG1uZlN0G",
    "outputId": "4eafa5cc-fa7d-4b99-9808-d14acc73f1cd"
   },
   "source": [
    "# Muss man in Google Colab einkommentieren\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "eaGId4_SkDd4",
    "outputId": "1e859d04-a320-4e26-9f32-fdfd032733ee"
   },
   "source": [
    "\n",
    "import re\n",
    "import sklearn\n",
    "import string\n",
    "import nltk\n",
    "from nltk import tokenize as tk\n",
    "import transformers as ppb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from enum import Enum\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import pipeline\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "print(pipeline('sentiment-analysis')('we love you'))\n",
    "\n",
    "\n",
    "# Man kann auch mehrere hintereinander machen\n",
    "\n",
    "class ColumnUser:\n",
    "    def set_column_to_use(self, column_name):\n",
    "        pass\n",
    "\n",
    "\n",
    "class ColumnTransformer(ColumnUser):\n",
    "    def set_column_to_transform(self, column_to_transform):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TextToSentenceTransformer(BaseEstimator, TransformerMixin, ColumnTransformer):\n",
    "    def __init__(self, column_to_transform, new_column_name, filename='logs/text_to_sentence_transformer.error'):\n",
    "        self.column_to_transform = column_to_transform\n",
    "        self.new_column_name = new_column_name\n",
    "        self.log_file = filename\n",
    "\n",
    "    def set_column_to_transform(self, column_to_transform):\n",
    "        self.column_to_transform = column_to_transform\n",
    "\n",
    "    def set_column_to_use(self, column_name):\n",
    "        self.new_column_name = column_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.column_to_transform and self.column_to_transform in data.columns:\n",
    "            return pd.DataFrame({self.new_column_name: self.split_text_in_sentences(data)})\n",
    "        else:\n",
    "            self.log_error(\n",
    "                f'no column with name {self.column_to_transform} in dataframe.\\nGiving back original dataframe.')\n",
    "            return data\n",
    "\n",
    "    def split_text_in_sentences(self, data) -> List[str]:\n",
    "        texts = data[self.column_to_transform].tolist()\n",
    "        sentences = list()\n",
    "        for text in texts:\n",
    "            # sentences_in_text = [e + delimiter for e in text.split(delimiter) if e]\n",
    "            sentences_in_text = [e for e in tk.sent_tokenize(str(text)) if e]\n",
    "            sentences += sentences_in_text\n",
    "\n",
    "        print(sentences[0])\n",
    "        return sentences\n",
    "\n",
    "    def log_error(self, description):\n",
    "        with open(self.log_file, 'a', encoding='utf-8') as file:\n",
    "            file.write('#------------------------------------------------------------------------------------------\\n')\n",
    "            file.write(f'{datetime.now().strftime(\"%b-%d-%Y %H:%M:%S\")}\\n')\n",
    "            file.write(f'\\t{description}\\n')\n",
    "            file.write(\n",
    "                '#------------------------------------------------------------------------------------------\\n\\n\\n')\n",
    "\n",
    "\n",
    "class BertTransformer(BaseEstimator, TransformerMixin, ColumnUser):\n",
    "\n",
    "    def __init__(self, column, batchsize=10):\n",
    "        self.column = column\n",
    "        self.batch_size = batchsize\n",
    "        model_class, tokenizer_class, pretrained_weights = (\n",
    "            ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "        # Load pretrained model/tokenizer\n",
    "\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "        self.model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "    def set_column_to_use(self, column_name):\n",
    "        self.column = column_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "\n",
    "        features = list()\n",
    "        row_count = data.shape[0]\n",
    "        counter = 0\n",
    "        start_index = 0\n",
    "        while start_index < row_count:\n",
    "            d = data.loc[start_index: start_index + self.batch_size]\n",
    "            feature = self.embedding(d)\n",
    "            features.extend(feature)\n",
    "            counter += 1\n",
    "            start_index += self.batch_size + 1 # das plus 1 kommt daher, dass bei den pandas Dataframes start und end index inklusive sind\n",
    "            if counter % 10 == 0:\n",
    "                print(f'{min(100.0, round(((start_index / row_count) * 100), 2))}% done')\n",
    "\n",
    "        return (data, features)\n",
    "\n",
    "    def embedding(self, data):\n",
    "        dataList = data[self.column].tolist()\n",
    "        dataList = list((str(s) for s in dataList))\n",
    "        tokenized = []\n",
    "\n",
    "        for s in dataList:\n",
    "            t = self.tokenizer.encode(s, add_special_tokens=True)\n",
    "            tokenized.append(t)\n",
    "\n",
    "        # Padding hinzufügen\n",
    "        max_len = 0\n",
    "        for i in tokenized:\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "\n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
    "\n",
    "        # Maske erstellen, um das Padding bei der Verarbeitung zu filtern\n",
    "        mask = np.where(padded != 0, 1, 0)\n",
    "        mask.shape\n",
    "\n",
    "        # mache padded Array und Maske zu einem Tensor\n",
    "        # Tensor = mehrdimensionale Matrix mit einheitlichem Datentyp\n",
    "        input = torch.tensor(padded).to(torch.long).long()\n",
    "        mask = torch.tensor(mask).to(torch.long).long()\n",
    "\n",
    "        # gib unser Zeug an BERT\n",
    "        # no_grad = Angabe zur Simplifikation des Rechenvorgangs\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input, attention_mask=mask)\n",
    "\n",
    "        # nur die erste Spalte auslesen = von BERT geschriebene Kennwerte\n",
    "        features = output[0][:, 0, :].numpy()\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "class PreprocessorTransformer(BaseEstimator, TransformerMixin, ColumnUser):\n",
    "\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def set_column_to_use(self, column_name):\n",
    "        self.column = column_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_and_features, y=None):\n",
    "        print('Starting with preprocessing')\n",
    "        data, features = data_and_features\n",
    "        sentences = data[self.column].tolist()\n",
    "        sentences = list((str(s) for s in sentences))\n",
    "\n",
    "        # muss vom generator object zurück zur liste gemacht werden\n",
    "        sentences = list((s.lower() for s in sentences))\n",
    "\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        sentences = [s.translate(table) for s in sentences]\n",
    "\n",
    "        sentences = [re.sub(r'\\d+', 'num', s) for s in sentences]\n",
    "\n",
    "        stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        sentences = [[word for word in s.split() if word not in stopwords] for s in sentences]\n",
    "        return (sentences, features)\n",
    "\n",
    "\n",
    "class SentimentOpinionValueCalculatorSingleValueTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, dict_name):\n",
    "        self.dict_name = dict_name\n",
    "        df = pd.read_csv(dict_name, sep=';')\n",
    "        self.value_dict = pd.Series(df.value.values, index=df.word).to_dict()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, sentences_and_features):\n",
    "        print('Starting with sentiment value calculation')\n",
    "        sentiment_opinion_scores = []\n",
    "        sentences, features = sentences_and_features\n",
    "        counter = 0\n",
    "        count_of_sentences = len(sentences)\n",
    "        for sentence in sentences:\n",
    "            word_count = len(sentence)\n",
    "            sentiment_opinion_score = 0\n",
    "            if word_count > 0:\n",
    "                for word in sentence:\n",
    "                    if word in self.value_dict:\n",
    "                        sentiment_opinion_score = sentiment_opinion_score + self.value_dict[word]\n",
    "                sentiment_opinion_score = sentiment_opinion_score / word_count\n",
    "            else:\n",
    "                sentiment_opinion_score = 0\n",
    "            sentiment_opinion_scores.append([sentiment_opinion_score])\n",
    "            counter += 1\n",
    "            if counter % 10 == 0:\n",
    "                print(f'{min(100.0, round(((counter / count_of_sentences) * 100), 2))}% of sentences done')\n",
    "\n",
    "        # TEST\n",
    "        print(len(sentiment_opinion_scores))\n",
    "        print(len(features))\n",
    "        for i in range(len(sentiment_opinion_scores)):\n",
    "            features[i] = features[i] + (sentiment_opinion_scores[i])\n",
    "        print('starting with classification')\n",
    "        return features\n",
    "\n",
    "\n",
    "class SentimentOpinionValueCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, value_file_name):\n",
    "        df = pd.read_csv(value_file_name, sep=';')\n",
    "        self.value_dict = pd.Series(df.value.values, index=df.word).to_dict()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, sentences_and_features):\n",
    "        sentiment_opinion_scores = []\n",
    "        sentences, features = sentences_and_features\n",
    "        for sentence in sentences:\n",
    "            word_count = len(sentence)\n",
    "            sentiment_opinion_score = 0\n",
    "            if word_count > 0:\n",
    "                for word in sentence:\n",
    "                    if word in self.value_dict:\n",
    "                        sentiment_opinion_score += 1\n",
    "            sentiment_opinion_scores.append([sentiment_opinion_score])\n",
    "        for i in range(len(sentiment_opinion_scores)):\n",
    "            features[i] = features[i] + (sentiment_opinion_scores[i][0])\n",
    "        return features\n",
    "\n",
    "\n",
    "class PipelineRunner:\n",
    "\n",
    "    def __init__(self, dict_file, training_file, test_file, log_file='results/results_with_correct_input.log'):\n",
    "        self.dict_file = dict_file\n",
    "        self.log_file = log_file\n",
    "        self.data_training = pd.read_excel(training_file, sheet_name='sentences')\n",
    "        # self.data_training.drop(\n",
    "        #     ['SUBJindl', 'SUBJsrce', 'SUBJrhet', 'SUBJster', 'SUBJspee', 'SUBJinspe', 'SUBJprop', 'SUBJpolit'],\n",
    "        #     axis=1,\n",
    "        #     inplace=True)\n",
    "        self.data_test = pd.read_excel(test_file, sheet_name='sentences')\n",
    "        # self.data_test.drop(\n",
    "        #     ['SUBJindl', 'SUBJsrce', 'SUBJrhet', 'SUBJster', 'SUBJspee', 'SUBJinspe', 'SUBJprop', 'SUBJpolit'],\n",
    "        #     axis=1,\n",
    "        #     inplace=True)\n",
    "        self.pipeline = None\n",
    "        self.transformer_name_dict = {\n",
    "                                      TextToSentenceTransformer.__name__: \"ts\",\n",
    "                                      BertTransformer.__name__: \"bert\",\n",
    "                                      PreprocessorTransformer.__name__: \"prepro\",\n",
    "                                      SentimentOpinionValueCalculatorSingleValueTransformer.__name__: \"sentval\",\n",
    "                                      SentimentOpinionValueCounterTransformer.__name__: \"sentcount\"\n",
    "                                      }\n",
    "        self.estimator_name_dict = {\n",
    "            LogisticRegression.__name__: 'log_reg',\n",
    "            GaussianNB.__name__: 'gau_nb',\n",
    "            BernoulliNB.__name__: 'ber_nb'\n",
    "        }\n",
    "\n",
    "    def prepare_pipeline(self, data_column, estimator_type, transformer_types_list):\n",
    "\n",
    "        if not self.pipeline:\n",
    "            raise RuntimeError(\"Don't call this function directly use make_pipeline\")\n",
    "\n",
    "        print(f'Starting fitting: {estimator_type}')\n",
    "        accuracy, f1, recall, precision = self.fit_and_predict_and_calculate_accuracy_pipe(data_column)\n",
    "        description = f'\\tUsed estimator: {estimator_type}\\n'\n",
    "        description += f'\\tUsed transformers: {\", \".join(transformer_types_list)}\\n'\n",
    "        description += f'\\tColumn: {data_column}\\n'\n",
    "        self.write_result_to_file(accuracy, f1, recall, precision, description)\n",
    "\n",
    "    def prepare_pipeline_confidence(self, data_column, estimator_type, transformer_types_list):\n",
    "\n",
    "        if not self.pipeline:\n",
    "            raise RuntimeError(\"Don't call this function directly use make_pipeline\")\n",
    "\n",
    "        print(f'Starting fitting: {estimator_type}')\n",
    "        result = self.fit_and_predict_and_calculate_confidence_pipe(data_column)\n",
    "        for threshhold, accuracy, f1, recall, precision in result:\n",
    "            description = f'\\tUsed estimator: {estimator_type}\\n'\n",
    "            description += f'\\tUsed transformers: {\", \".join(transformer_types_list)}\\n'\n",
    "            # description += f'\\tonly 60percent confidence \\n'\n",
    "            description += f'\\tColumn: {data_column}\\n'\n",
    "            self.write_result_to_file_confidence(threshhold, accuracy, f1, recall, precision, description)\n",
    "\n",
    "    def make_pipeline(self, transformer_list, estimator,\n",
    "                      data_column, param_gird, classifier_description='',\n",
    "                      dir_path='', force_fitting=False):\n",
    "\n",
    "        classifier_file = self.create_pipe_line_name(transformer_list, estimator, classifier_description, data_column, dir_path)\n",
    "        if force_fitting or not os.path.exists(classifier_file):\n",
    "            print('No classfier found for your configuration or force_fitting=True. Creating new one and saving it.')\n",
    "            clf = GridSearchCV(estimator=estimator, param_grid=param_gird, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "            self.pipeline = sklearn.pipeline.Pipeline(\n",
    "                [(f'stage: {self.transformer_name_dict[type(transformer_list[index]).__name__]}',\n",
    "                  transformer_list[index]) for index in range(len(transformer_list))] +\n",
    "                [('clf', clf)]\n",
    "            )\n",
    "            self.prepare_pipeline(data_column, type(estimator).__name__, [type(t).__name__ for t in transformer_list])\n",
    "            print(f'Saving classifier to file {classifier_file}')\n",
    "            self.save_classifier(classifier_file)\n",
    "        else:\n",
    "            print(f'Classifier exists. Loaded from file {classifier_file}')\n",
    "            self.load_classifier(classifier_file)\n",
    "\n",
    "        return self.pipeline\n",
    "\n",
    "    def make_pipeline_confidence(self, transformer_list, estimator,\n",
    "                                data_column, param_gird, classifier_description='',\n",
    "                                dir_path='', force_fitting=False):\n",
    "\n",
    "        classifier_file = self.create_pipe_line_name(transformer_list, estimator, classifier_description, data_column, dir_path)\n",
    "        if force_fitting or not os.path.exists(classifier_file):\n",
    "            print('No classfier found for your configuration or force_fitting=True. Creating new one and saving it.')\n",
    "            clf = GridSearchCV(estimator=estimator, param_grid=param_gird, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "            self.pipeline = sklearn.pipeline.Pipeline(\n",
    "                [(f'stage: {self.transformer_name_dict[type(transformer_list[index]).__name__]}',\n",
    "                  transformer_list[index]) for index in range(len(transformer_list))] +\n",
    "                [('clf', clf)]\n",
    "            )\n",
    "            self.prepare_pipeline_confidence(data_column, type(estimator).__name__, [type(t).__name__ for t in transformer_list])\n",
    "            print(f'Saving classifier to file {classifier_file}')\n",
    "            self.save_classifier(classifier_file)\n",
    "        else:\n",
    "            print(f'Classifier exists. Loaded from file {classifier_file}')\n",
    "            self.load_classifier(classifier_file)\n",
    "\n",
    "        return self.pipeline\n",
    "\n",
    "\n",
    "    def create_pipe_line_name(self, transformer_list, estimator, classifier_description, data_column, dir_path=''):\n",
    "        names = [self.transformer_name_dict[type(transformer).__name__] for transformer in transformer_list]\n",
    "        description = ''\n",
    "        if classifier_description:\n",
    "            description = '_' + classifier_description\n",
    "        if dir_path and not dir_path.endswith('/') and not dir_path.endswith('\\\\'):\n",
    "            dir_path += '/'\n",
    "        return dir_path + f'classifier/{self.estimator_name_dict[type(estimator).__name__]}_with_{\"_\".join(names)}{description}_{data_column}_pipeline.joblib.plk'\n",
    "\n",
    "    def load_classifier(self, filename):\n",
    "        self.pipeline = joblib.load(filename)\n",
    "\n",
    "    def save_classifier(self, filename):\n",
    "        joblib.dump(self.pipeline, filename)\n",
    "\n",
    "    def fit_and_predict_and_calculate_accuracy_pipe(self, data_column):\n",
    "        print('Start fiting')\n",
    "        self.pipeline.fit(self.data_training, self.data_training[data_column].to_numpy())\n",
    "        print('Start prediction')\n",
    "        start = time.time()\n",
    "        y_pred_pipe = self.pipeline.predict(self.data_test)\n",
    "        # self.safe_prediction(y_pred_pipe, data_column)\n",
    "        end = time.time()\n",
    "        print(f'time needed: {end - start}')\n",
    "\n",
    "        #return accuracy_score(self.data_test[data_column].to_numpy(), y_pred_pipe)\n",
    "\n",
    "        acc = accuracy_score(self.data_test[data_column].to_numpy(), y_pred_pipe)\n",
    "        f1 = f1_score(self.data_test[data_column].to_numpy(), y_pred_pipe, average='weighted')\n",
    "        rec = recall_score(self.data_test[data_column].to_numpy(), y_pred_pipe, average='weighted')\n",
    "        precision = precision_score(self.data_test[data_column].to_numpy(), y_pred_pipe, average='weighted')\n",
    "        return acc, f1, rec, precision\n",
    "\n",
    "    def fit_and_predict_and_calculate_confidence_pipe(self, data_column):\n",
    "        print('Start fiting')\n",
    "        self.pipeline.fit(self.data_training, self.data_training[data_column].to_numpy())\n",
    "        print('Start prediction')\n",
    "\n",
    "        # predicte die confidence für jedes Item\n",
    "        y_pred_pipe = self.pipeline.predict_proba(self.data_test)\n",
    "        df = pd.DataFrame(data=y_pred_pipe, columns=['confidence class 0', 'confidence class 1'])\n",
    "\n",
    "        # predicte die Klasse für jedes Item\n",
    "        y_pred_pipe2 = self.pipeline.predict(self.data_test)\n",
    "        df['prediction'] = y_pred_pipe2\n",
    "\n",
    "        # schreibe die confidence der predicted class in die Spalte max confidence\n",
    "        df['max confidence'] = df[['confidence class 0', 'confidence class 1']].max(axis=1)\n",
    "\n",
    "        # lösche spalten confidence class 0 und confidence class 1\n",
    "        df.drop(['confidence class 0', 'confidence class 1'], axis=1)\n",
    "\n",
    "        # neuen dataframe erstellen mit den richtigen Klassen, den predicted classes und den dazugehörigen confidences\n",
    "        df2 = pd.DataFrame(data=self.data_test[data_column].to_numpy(), columns=['real sentiment label'])\n",
    "        df2['prediction'] = df['prediction']\n",
    "        df2['confidence'] = df['max confidence']\n",
    "\n",
    "        result_list = list()\n",
    "        threshholds = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "        for threshhold in threshholds:\n",
    "            # alle Zeilen aussortieren, die eine confidence unter 60 haben\n",
    "            df3 = pd.DataFrame({'real sentiment label': [100], 'prediction': [100], 'confidence': [100]})\n",
    "            for ind in df2.index:\n",
    "                if (df2['confidence'][ind]) > threshhold:\n",
    "                    item = pd.DataFrame({'real sentiment label': [df2['real sentiment label'][ind]],\n",
    "                                         'prediction': [df2['prediction'][ind]],\n",
    "                                         'confidence': [df2['confidence'][ind]]})\n",
    "                    df3 = df3.append(item, ignore_index=True)\n",
    "\n",
    "                    # dummy Zeile wieder löschen\n",
    "            df3.drop(df3.head(1).index, inplace=True)\n",
    "\n",
    "            # zum Schluss alles in die excel speichern\n",
    "            # df.to_excel(f'results/max_confidence_proba_{threshhold}_{data_column}.xlsx', index=False)\n",
    "            # df2.to_excel(f'results/classes_and_confidence_proba__{threshhold}_{data_column}.xlsx', index=False)\n",
    "            # df3.to_excel(f'results/confidence_proba__{threshhold}_{data_column}.xlsx', index=False)\n",
    "\n",
    "            acc = accuracy_score(df3['real sentiment label'], df3['prediction'])\n",
    "            f1 = f1_score(df3['real sentiment label'], df3['prediction'], average='weighted')\n",
    "            rec = recall_score(df3['real sentiment label'], df3['prediction'], average='weighted')\n",
    "            precision = precision_score(df3['real sentiment label'], df3['prediction'], average='weighted')\n",
    "            result_list.append((threshhold, acc, f1, rec, precision))\n",
    "\n",
    "        return result_list\n",
    "\n",
    "    def safe_prediction(self, prediction, data_column):\n",
    "        pred_data_frame = self.data_test.copy(deep=True)\n",
    "        pred_data_frame[f'prediction_{data_column}'] = prediction\n",
    "        pred_data_frame.to_excel(f'results/prediction_for_{data_column}.xlsx', index=False)\n",
    "        pass\n",
    "\n",
    "    def write_result_to_file(self, accuracy, f1, recall, precision, description):\n",
    "        with open(self.log_file, 'a', encoding='utf-8') as file:\n",
    "            file.write('#------------------------------------------------------------------------------------------\\n')\n",
    "            file.write(f'{datetime.now().strftime(\"%b-%d-%Y %H:%M:%S\")}\\n')\n",
    "            file.write(f'{description}\\n')\n",
    "            file.write(f'\\t\\tAccuracy: {accuracy}\\n')\n",
    "            file.write(f'\\t\\tF1 Score: {f1}\\n')\n",
    "            file.write(f'\\t\\tRecall Score: {recall}\\n')\n",
    "            file.write(f'\\t\\tPrecision Score: {precision}\\n')\n",
    "            file.write('#------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "    def write_result_to_file_confidence(self, threshhold, accuracy, f1, recall, precision, description,):\n",
    "        with open(self.log_file, 'a', encoding='utf-8') as file:\n",
    "            file.write('#------------------------------------------------------------------------------------------\\n')\n",
    "            file.write(f'{datetime.now().strftime(\"%b-%d-%Y %H:%M:%S\")}\\n')\n",
    "            file.write(f'for threshhold: {threshhold}\\n')\n",
    "            file.write(f'{description}\\n')\n",
    "            file.write(f'\\t\\tAccuracy: {accuracy}\\n')\n",
    "            file.write(f'\\t\\tF1 Score: {f1}\\n')\n",
    "            file.write(f'\\t\\tRecall Score: {recall}\\n')\n",
    "            file.write(f'\\t\\tPrecision Score: {precision}\\n')\n",
    "            file.write('#------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "    def predict_data(self, data_file_name, column_to_transform=None, new_column_name=None, batch_size=1, result_for_column='', log_file=f'results/results_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log'):\n",
    "        data_validation = pd.read_csv(data_file_name)\n",
    "\n",
    "        if column_to_transform is not None:\n",
    "            for obj in self.pipeline.named_steps.values():\n",
    "                if issubclass(type(obj), ColumnTransformer):\n",
    "                    obj.set_column_to_transform(column_to_transform)\n",
    "\n",
    "        if new_column_name is not None:\n",
    "            for obj in self.pipeline.named_steps.values():\n",
    "                if issubclass(type(obj), ColumnUser):\n",
    "                    obj.set_column_to_use(new_column_name)\n",
    "\n",
    "        # with open(log_file, 'a', encoding='utf-8') as f:\n",
    "        #     f.write('#-------------------------------------------------')\n",
    "        #     f.write(f'result for column {result_for_column}:\\n')\n",
    "        #\n",
    "        # row_count = data_validation.shape[0]\n",
    "        # counter = 0\n",
    "        # start_index = 0\n",
    "        #\n",
    "        # while start_index < row_count:\n",
    "        #\n",
    "        #     output = self.pipeline.predict(data_validation.loc[start_index: start_index + batch_size])\n",
    "        #     with open(log_file, 'a', encoding='utf-8') as f:\n",
    "        #         f.write(f'{output.tolist()}')\n",
    "        #         f.write('\\n')\n",
    "        #     start_index += batch_size + 1 # das plus 1 kommt daher, dass bei den pandas Dataframes start und end index inklusive sind\n",
    "        #     counter += 1\n",
    "        #     if counter % 10 == 0:\n",
    "        #         print(f'{min(100.0, round((((start_index) / row_count) * 100), 2))}% done')\n",
    "        #\n",
    "        # with open(log_file, 'a', encoding='utf-8') as f:\n",
    "        #     f.write('#-------------------------------------------------\\n\\n\\n\\n\\n')\n",
    "\n",
    "        output = self.pipeline.predict(data_validation)\n",
    "        with open(log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write('#-------------------------------------------------')\n",
    "            f.write(f'result for column {result_for_column}:\\n')\n",
    "            f.write(f'{output.tolist()}')\n",
    "            f.write('\\n')\n",
    "            f.write('#-------------------------------------------------\\n\\n\\n\\n\\n')\n"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pasca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pasca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "    dir_path = ''\n",
    "    # dict_file = dir_path + 'AFINN-both-abs.csv'\n",
    "    dict_file = dir_path + 'sentiment_dict.csv'\n",
    "\n",
    "    training_file = dir_path + 'datasetSentimentSRF_train.xlsx'\n",
    "    # training_file = dir_path + 'TrainingdataNew_train.xlsx'\n",
    "\n",
    "    test_file = dir_path + 'datasetSentimentSRF_test.xlsx'\n",
    "    # test_file = dir_path + 'TrainingdataNew_test.xlsx'\n",
    "\n",
    "    transformers_list = [TextToSentenceTransformer('text', 'Sentence'),\n",
    "                         BertTransformer('Sentence', batchsize=10),\n",
    "                         PreprocessorTransformer('Sentence'),\n",
    "                         SentimentOpinionValueCalculatorSingleValueTransformer(dict_file)]\n",
    "\n",
    "    pipeline_runner = PipelineRunner(dict_file, training_file, test_file, log_file=dir_path + 'results/results_for_different_threshholds.log')\n",
    "    Cs = np.logspace(-6, 6, 200)\n",
    "    max_iter = 500\n",
    "    # bert = BertTransformer('Sentence', batchsize=100)\n",
    "    # bert.transform(pd.read_excel('datasetSingleSentences.xlsx', sheet_name='Sheet1'))\n",
    "    # log_reg_subjopin = LogisticRegression(max_iter=max_iter)\n",
    "    # pipeline_runner.make_pipeline(transformers_list, log_reg_subjopin, 'SUBJopin01', dict(C=Cs), dir_path=dir_path, classifier_description='probability')\n",
    "\n",
    "    # pipeline_runner.predict_data(data_file_name=dir_path + 'MBFC-sentences-Dataset.csv',\n",
    "    #                              result_for_column='SUBJopin',\n",
    "    #                              log_file=dir_path + f'results/mbfc_sentences_results_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}_SUBJopin.log',\n",
    "    #                              new_column_name='sentences')\n",
    "\n",
    "    # log_reg_subjlang = LogisticRegression(max_iter=max_iter)\n",
    "    # pipeline_runner.make_pipeline(transformers_list, log_reg_subjlang, 'SUBJlang01', dict(C=Cs), dir_path=dir_path, classifier_description='probability')\n",
    "    #\n",
    "    # pipeline_runner.predict_data(data_file_name=dir_path + 'MBFC-sentences-Dataset.csv',\n",
    "    #                              result_for_column='SUBJlang',\n",
    "    #                              log_file=dir_path + f'results/mbfc_sentences_results_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}_SUBJlang.log',\n",
    "    #                              new_column_name='sentences')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZC9OLXzMo4sC"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}